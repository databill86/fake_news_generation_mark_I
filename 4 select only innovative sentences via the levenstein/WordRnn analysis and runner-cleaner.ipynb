{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/kangeunsu/.local/lib/python3.5/site-packages\n",
      "Requirement already satisfied: six in /share/apps/lib/python3.5/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: fuzzywuzzy[speedup] in /home/kangeunsu/.local/lib/python3.5/site-packages\n",
      "Requirement already satisfied: python-levenshtein>=0.12; extra == \"speedup\" in /home/kangeunsu/.local/lib/python3.5/site-packages (from fuzzywuzzy[speedup])\n",
      "Requirement already satisfied: setuptools in /share/apps/lib/python3.5/site-packages (from python-levenshtein>=0.12; extra == \"speedup\"->fuzzywuzzy[speedup])\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk --user\n",
    "!pip3 install fuzzywuzzy[speedup] --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these needed to be installed\n",
    "import nltk.data\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models:\n",
      "['sample.py', 'save', 'model.py', 'save_testrun', '__pycache__', '.git', 'output_america_9k', 'beam.pyc', 'how_to.txt', 'GENERATED__save_100ep_korea_plus_trump_21456_longer_than_400chars.txt', 'last_plot.png', 'README.md', 'beam.py', 'output_china', '.travis.yml', 'output_america', 'save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs.txt', 'june15-2018', 'save_first_decent_model_subdataset', 'save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs__distances.png', '.gitignore', 'save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs__distances.pdf', 'logs', 'save_korea_texts_3064_longer_than_400chars__W16Beam_paragraphs.txt', 'save_korea_texts_3064_longer_than_400chars', 'model.pyc', 'train.py', 'save_50ep_korea_texts_3064_longer_than_400chars', 'save_100ep_korea_plus_trump_21456_longer_than_400chars', 'save_first_decent_model_subdataset__W4Beam_paragraphs.txt', 'save_test_run', 'save_50ep_korea_plus_trump_21456_longer_than_400chars', 'data', 'tests', 'LICENSE.md', 'output_rohingya', 'save_first_decent_model_subdataset__noBeam_paragraphs.txt', 'utils.py', 'save_100ep_korea_minus_trump_2326_longer_than_400chars', 'save_korea_texts_3064_longer_than_400chars_128x2M', 'save_korea_texts_3064_longer_than_400chars__noBeam_paragraphs.txt', 'output_fakenews', 'output_china_50ep', 'GENERATED__output_china_50ep_china_texts_5245.txt', 'save_korea_texts_3064_longer_than_400chars__W4_60paragraphs.txt', 'utils.pyc']\n",
      "\n",
      "\n",
      "save\n",
      "save_testrun\n",
      "GENERATED__save_100ep_korea_plus_trump_21456_longer_than_400chars.txt\n",
      "save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs.txt\n",
      "save_first_decent_model_subdataset\n",
      "save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs__distances.png\n",
      "save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs__distances.pdf\n",
      "save_korea_texts_3064_longer_than_400chars__W16Beam_paragraphs.txt\n",
      "save_korea_texts_3064_longer_than_400chars\n",
      "save_50ep_korea_texts_3064_longer_than_400chars\n",
      "save_100ep_korea_plus_trump_21456_longer_than_400chars\n",
      "save_first_decent_model_subdataset__W4Beam_paragraphs.txt\n",
      "save_test_run\n",
      "save_50ep_korea_plus_trump_21456_longer_than_400chars\n",
      "save_first_decent_model_subdataset__noBeam_paragraphs.txt\n",
      "save_100ep_korea_minus_trump_2326_longer_than_400chars\n",
      "save_korea_texts_3064_longer_than_400chars_128x2M\n",
      "save_korea_texts_3064_longer_than_400chars__noBeam_paragraphs.txt\n",
      "save_korea_texts_3064_longer_than_400chars__W4_60paragraphs.txt\n",
      "\n",
      "\n",
      "Datasets:\n",
      "['tinyshakespeare', 'test2_america', 'test2_china', 'korea_txt', 'test2_fakenews', 'korea_texts_3318', 'korea_texts_3064_longer_than_400chars', 'test2_rohingya', 'korea_plus_trump_21456_texts_longer_than_400chars', 'test2_rohingya_less', 'korea_texts_3175_longer_than_240chars', 'korea_minus_trump_2326_texts_longer_than_400chars', 'test2_america_9k']\n",
      "In directory: /home/kangeunsu/word-rnn-tensorflow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re, string\n",
    "\n",
    "def ls(path):\n",
    "    ls = [f for f in os.listdir(path)]\n",
    "    print(ls)\n",
    "    return ls\n",
    "    \n",
    "def cd(path):\n",
    "    os.chdir(path)\n",
    "    cwd = os.getcwd()\n",
    "    print(\"In directory:\", cwd)\n",
    "\n",
    "print(\"Saved models:\")\n",
    "dirs = ls(\"/home/kangeunsu/word-rnn-tensorflow\")\n",
    "\n",
    "print(\"\\n\")\n",
    "for dir in dirs:\n",
    "    if \"save\" in dir:\n",
    "        print(dir)\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(\"Datasets:\")\n",
    "ls(\"/home/kangeunsu/word-rnn-tensorflow/data\")\n",
    "\n",
    "cd(\"/home/kangeunsu/word-rnn-tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'save_100ep_korea_plus_trump_21456_longer_than_400chars'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These finished:\n",
    "\"save_korea_texts_3064_longer_than_400chars\"\n",
    "\"save_100ep_korea_minus_trump_2326_longer_than_400chars\"\n",
    "\"save_korea_texts_3064_longer_than_400chars_128x2M\"\n",
    "\n",
    "\"save_50ep_korea_plus_trump_21456_longer_than_400chars\"\n",
    "\"save_100ep_korea_plus_trump_21456_longer_than_400chars\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample.py', 'save', 'model.py', 'save_testrun', '__pycache__', '.git', 'output_america_9k', 'beam.pyc', 'how_to.txt', 'GENERATED__save_100ep_korea_plus_trump_21456_longer_than_400chars.txt', 'last_plot.png', 'README.md', 'beam.py', 'output_china', '.travis.yml', 'output_america', 'save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs.txt', 'june15-2018', 'save_first_decent_model_subdataset', 'save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs__distances.png', '.gitignore', 'save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs__distances.pdf', 'logs', 'save_korea_texts_3064_longer_than_400chars__W16Beam_paragraphs.txt', 'save_korea_texts_3064_longer_than_400chars', 'model.pyc', 'train.py', 'save_50ep_korea_texts_3064_longer_than_400chars', 'save_100ep_korea_plus_trump_21456_longer_than_400chars', 'save_first_decent_model_subdataset__W4Beam_paragraphs.txt', 'save_test_run', 'save_50ep_korea_plus_trump_21456_longer_than_400chars', 'data', 'tests', 'LICENSE.md', 'output_rohingya', 'save_first_decent_model_subdataset__noBeam_paragraphs.txt', 'utils.py', 'save_100ep_korea_minus_trump_2326_longer_than_400chars', 'save_korea_texts_3064_longer_than_400chars_128x2M', 'save_korea_texts_3064_longer_than_400chars__noBeam_paragraphs.txt', 'output_fakenews', 'output_china_50ep', 'GENERATED__output_china_50ep_china_texts_5245.txt', 'save_korea_texts_3064_longer_than_400chars__W4_60paragraphs.txt', 'utils.pyc']\n"
     ]
    }
   ],
   "source": [
    "ls(\"/home/kangeunsu/word-rnn-tensorflow/\")\n",
    "\n",
    "\"\"\"\n",
    "save_first_decent_model_subdataset__noBeam_paragraphs.txt\n",
    "save_first_decent_model_subdataset__W4Beam_paragraphs.txt\n",
    "save_korea_texts_3064_longer_than_400chars_128x2M__W460paragraphs.txt\n",
    "save_korea_texts_3064_longer_than_400chars__noBeam_paragraphs.txt\n",
    "save_korea_texts_3064_longer_than_400chars__W16Beam_paragraphs.txt\n",
    "save_korea_texts_3064_longer_than_400chars__W4_60paragraphs.txt\n",
    "\n",
    "GENERATED__save_100ep_korea_plus_trump_21456_longer_than_400chars.txt\n",
    "\"\"\"\n",
    "\n",
    "file = \"GENERATED__save_100ep_korea_plus_trump_21456_longer_than_400chars.txt\"\n",
    "with open(file) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "content = \"\\n\".join(content)\n",
    "\n",
    "#content = content.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 334625 letters....\n",
      "coming, nice and new and “smart!” You shouldn’t be partners with a Gas Killing Animal who kills his people and enjoys it! — Donald J. Trump (@realDonaldTrump) April 11, 2018 The White House did not immediately respond to a request for comment. The White House did not immediately respond to a request for comment. The White House said that the United States would prematurely withdraw from Syria,” the official said. At the end of the day, U.S. President Donald Trump said on Twitter that the U.S. wo\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated\",len(content),\"letters....\")\n",
    "print(content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load source text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vocab.pkl', 'data.npy', 'input.txt']\n",
      "Loaded 122230800 chars Donald Trump proposes 'IQ tests' face-off with Sec\n"
     ]
    }
   ],
   "source": [
    "data_file = \"/home/kangeunsu/word-rnn-tensorflow/data/korea_plus_trump_21456_texts_longer_than_400chars/\"\n",
    "ls(data_file)\n",
    "\n",
    "whole_text_path = \"/home/kangeunsu/word-rnn-tensorflow/data/korea_plus_trump_21456_texts_longer_than_400chars/input.txt\"\n",
    "with open(whole_text_path, 'r') as content_file:\n",
    "    whole_text = content_file.read()\n",
    "    \n",
    "print(\"Loaded\",len(whole_text),\"chars\", whole_text[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version which needs NLTK\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def string_to_sentences(paragraph):\n",
    "    return tokenizer.tokenize(paragraph)\n",
    "    # 106 481 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version which doesn't need NLTK\n",
    "#\n",
    "#def string_to_sentences(paragraph):\n",
    "#    sentenceEnders = re.compile('[.!?]')\n",
    "#    sentenceList = sentenceEnders.split(paragraph)\n",
    "#    return sentenceList\n",
    "#    # 142 924 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18929606 words\n",
      "775965 sentences\n"
     ]
    }
   ],
   "source": [
    "def string_to_words(s):\n",
    "    return [re.sub('^[{0}]+|[{0}]+$'.format(string.punctuation), '', w) for w in s.split()]\n",
    "\n",
    "#whole_text = whole_text.decode('utf-8')\n",
    "    \n",
    "full_word_list = string_to_words(whole_text)\n",
    "full_sentence_list = string_to_sentences(whole_text)\n",
    "#unique_words = list(set(word_list))\n",
    "print(len(full_word_list),\"words\")\n",
    "print(len(full_sentence_list),\"sentences\")\n",
    "#print(len(unique_words),\"unique\", unique_words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Cohen, Trump’s personal pit bull lawyer, is radioactive over his role in the Stormy Daniels affair and may face legal exposure of his own.\n",
      "===\n",
      "Both presidents used the military to perform another task.\n",
      "===\n",
      "Incumbents have a name identification advantage.\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(1, len(full_sentence_list), size=3)\n",
    "for i in idx:\n",
    "    print(full_sentence_list[i])\n",
    "    print(\"===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with file GENERATED__save_100ep_korea_plus_trump_21456_longer_than_400chars.txt now...\n",
      "parsed generated text as 1540  sentences.\n"
     ]
    }
   ],
   "source": [
    "#sample_paragraph = \"/n\".join(paragraphs)\n",
    "#sample_paragraph = paragraphs[0] + paragraphs[1] + paragraphs[2]\n",
    "#sample_paragraph = sample_paragraph[0:200]\n",
    "print(\"Working with file\", file, \"now...\")\n",
    "sample_paragraph = content\n",
    "\n",
    "sample_sentences = string_to_sentences(sample_paragraph)\n",
    "print(\"parsed generated text as\", len(sample_sentences), \" sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first few:\n",
    "#sample_sentences = sample_sentences[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suspicious sentences detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_sentences = {}\n",
    "\n",
    "similarities = []\n",
    "\n",
    "i = 0\n",
    "#for sentence in tqdm(sample_sentences):\n",
    "for sentence in sample_sentences:\n",
    "    closest_sentences[i] = []\n",
    "    \n",
    "    max_so_far = -1.0\n",
    "    closest_sentence = \"\"\n",
    "    \n",
    "    for close_sentence in full_sentence_list:\n",
    "        r = float(fuzz.ratio(sentence, close_sentence)) / 100.0\n",
    "        # SUPER SLOW >> r = (difflib.SequenceMatcher(None, sentence, close_sentence).ratio())\n",
    "        if r >= max_so_far:\n",
    "            max_so_far = r\n",
    "            closest_sentence = close_sentence\n",
    "    \n",
    "            closest_sentences[i].append([r, closest_sentence])\n",
    "    similarities.append(max_so_far)\n",
    "    \n",
    "    ###\n",
    "    \"\"\"\n",
    "    if max_so_far > 0.8:\n",
    "        print(i)\n",
    "        print(\"Generated sentence \",i,\" \\\"\", sentence, \"\\\" has closest:\")\n",
    "        print(max_so_far,\":\",closest_sentence)\n",
    "        print(\"-----------------\")\n",
    "    \"\"\"\n",
    "    if max_so_far < 0.7:\n",
    "        # print \"good enough\" sentences\n",
    "        print(sentence)\n",
    "    \n",
    "    \n",
    "    #for close in closest_sentences[i]:\n",
    "    #    print(close[0],\":\", close[1])\n",
    "    \n",
    "    i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.plot(similarities)\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.title(\"Levenstein distance to nearest sentence in the real dataset.\")\n",
    "# Two layer model with 256 neurons, beam search width 16\n",
    "plt.xlabel(\"Default model and sampling.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"/home/kangeunsu/ArtML/_Word_rnn_analysis/last__distances.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the best generated sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max score\n",
    "\n",
    "tmp_scores = []\n",
    "tmp_sentences = []\n",
    "tmp_close_sentences = []\n",
    "for i,sentence in enumerate(sample_sentences):\n",
    "    \n",
    "    max_so_far = -1.0\n",
    "    maximum_sentence=\"\"\n",
    "    #print(\"Sentence \",i,\" \\\"\", sentence, \"\\\" has closest:\")\n",
    "    for close in closest_sentences[i]:\n",
    "        #print(close[0],\":\", close[1])\n",
    "        if close[0]>max_so_far:\n",
    "            max_so_far=close[0] \n",
    "            maximum_sentence=sentence\n",
    "            closest_sentence=close[1]\n",
    "    \n",
    "    tmp_scores.append(max_so_far)\n",
    "    tmp_sentences.append(maximum_sentence)\n",
    "    tmp_close_sentences.append(closest_sentence)\n",
    "    #print(\"-----------------\")\n",
    "    \n",
    "sort_order = np.argsort(tmp_scores)\n",
    "tmp_scores = [tmp_scores[i] for i in sort_order]\n",
    "tmp_sentences = [tmp_sentences[i] for i in sort_order]\n",
    "tmp_close_sentences = [tmp_close_sentences[i] for i in sort_order]\n",
    "\n",
    "print(\"Most innovative sentences are: (sorted)\")\n",
    "for i in range(len(tmp_scores)):\n",
    "    print(tmp_scores[i], \":\", tmp_sentences[i])\n",
    "    print(\"~~~\",tmp_close_sentences[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need to restart after each call\n",
    "\n",
    "#kernel.restart()\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "These might need:\n",
    "#pip3 install python-Levenshtein --user\n",
    "#pip3 install nltk --user\n",
    "#pip3 install Distance --user\n",
    "#pip3 install jellyfish --user\n",
    "#pip3 install fuzzywuzzy[speedup] --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sample sentence.', 'Some text.']\n",
      "0.5436893203883495\n",
      "33\n",
      "33\n",
      "54\n",
      "50\n",
      "54\n",
      "0.5048543689320388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "print(tokenizer.tokenize(\"Sample sentence. Some text.\"))\n",
    "\n",
    "s1 = \"While you do not have a lot of different cases\"\n",
    "s2 = \"The US does not have a diplomatic presence in North Korea\"\n",
    "\n",
    "import Levenshtein\n",
    "print (Levenshtein.ratio(s1,s2))\n",
    "\n",
    "import distance\n",
    "print (distance.levenshtein(s1,s2))\n",
    "\n",
    "import jellyfish\n",
    "print (jellyfish.levenshtein_distance(s1,s2))\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "print (fuzz.ratio(s1,s2))\n",
    "print (fuzz.token_sort_ratio(s1,s2))\n",
    "print (fuzz.token_set_ratio(s1,s2))\n",
    "\n",
    "\n",
    "import difflib\n",
    "print (difflib.SequenceMatcher(None, s1,s2).ratio())\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5048543689320388\n"
     ]
    }
   ],
   "source": [
    "# Default python library only...\n",
    "# ... it's very slow though\n",
    "s1 = \"While you do not have a lot of different cases\"\n",
    "s2 = \"The US does not have a diplomatic presence in North Korea\"\n",
    "\n",
    "import difflib\n",
    "print (difflib.SequenceMatcher(None, s1,s2).ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
